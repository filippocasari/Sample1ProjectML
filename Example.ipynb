{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, recall_score, precision_score, accuracy_score, \\\n",
    "    precision_recall_curve, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import EDA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import Plotting\n",
    "from Features_Selection import feature_selection_kbest\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer, MinMaxScaler\n",
    "from Logistic_Regression import Logistic_regression\n",
    "from SVM_classifier import SVM_classifier\n",
    "\n",
    "count_features = False\n",
    "discretization_bool = True\n",
    "problem_is_binarized = False\n",
    "normalization = False\n",
    "standardization = False\n",
    "preproc = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# used algorithms : Logistic Regression, DecisionTree, Clustering (K-means for evaluate number of classes)\n",
    "\n",
    "\n",
    "def plot_metrics_for_each_features(names_cols, X, name_png):\n",
    "    figures = []\n",
    "    try:\n",
    "        os.makedirs(\"./plots\")\n",
    "    except FileExistsError:\n",
    "        # directory already exists\n",
    "        pass\n",
    "    for i in names_cols:\n",
    "        figure = sns.displot(X, x=i)\n",
    "        figures.append(figure)\n",
    "\n",
    "        figure.savefig(\"./plots/\" + str(i) + name_png)\n",
    "        plt.close()  # plot close per chiudere la finestra di plot, onde evitare troppi  (>20)\\\n",
    "        # ed avere un errore a Runtime\n",
    "\n",
    "\n",
    "\n",
    "def splitting_train_test(X, Y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, random_state=0, train_size=0.7\n",
    "    )\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "def select_best_features_with_kbest_log_regr(X, Y):\n",
    "    for i in range(3, 28):\n",
    "        X_new = feature_selection_kbest(X, Y, i)\n",
    "        X_train, X_test, Y_train, Y_test = splitting_train_test(X_new, Y)\n",
    "        title = \"Learning Curves with Logistic Regression (with select from model)\"\n",
    "        log_regr, accuracy_score, y_pred = Logistic_regression(X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "        Plotting.plot_lc_curve(X_train, Y_train, title, i)\n",
    "\n",
    "        # Plotting.plot_metrics_results(Y_test, y_pred, \"Logistic Regression\")\n",
    "\n",
    "\n",
    "def select_from_model(X, Y, clf):\n",
    "    feature_names = X.columns\n",
    "    X_train, X_test, Y_train, Y_test = splitting_train_test(X, Y)\n",
    "\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "    mask = model.get_support()  # list of booleans\n",
    "    new_features = []  # The list of your K best features\n",
    "\n",
    "    for bool, feature in zip(mask, feature_names):\n",
    "        if bool:\n",
    "            new_features.append(feature)\n",
    "    X_new = pd.DataFrame(data=model.transform(X), columns=new_features)\n",
    "    X_train, X_test, Y_train, Y_test = splitting_train_test(X_new, Y)\n",
    "    print(\" X with selection from model \\n\" + str(X_new))\n",
    "    title = \"Learning Curves with Logistic Regression (with select from model)\"\n",
    "    Plotting.plot_lc_curve(X_train, Y_train, title)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def analysis_dataset(df):\n",
    "    # EDA starting...\n",
    "    print(df)\n",
    "    # df = discr_fun(df)\n",
    "\n",
    "    # plt.hist(df['Baselinehistological staging'])\n",
    "\n",
    "    # show the balanced dataset\n",
    "    sns.displot(data=df['Baselinehistological staging'])\n",
    "    plt.show()\n",
    "\n",
    "    df.plot.scatter(x='RNA 12', y='RNA EF', c='Baselinehistological staging', logy=True, cmap='summer')\n",
    "    plt.show()\n",
    "    df.plot.scatter(x='RNA 12', y='RNA EOT', c='Baselinehistological staging', logy=True, cmap='autumn')\n",
    "    plt.show()\n",
    "    print(pd.crosstab(df['RNA 12'], df['Baselinehistological staging'], margins=True))\n",
    "    sns.countplot(x='RNA 12', hue='RNA EOT', data=df)\n",
    "    plt.show()\n",
    "    sns.boxplot(x='RNA EF', data=df)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def clustering(X, title):\n",
    "    inertia = []  # Squared Distance between Centroids and data points\n",
    "    for n in range(1, 11):\n",
    "        algorithm = (KMeans(n_clusters=n, init='k-means++', n_init=10, random_state=111,\n",
    "                            algorithm='elkan'))\n",
    "        algorithm.fit(X)\n",
    "        inertia.append(algorithm.inertia_)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(1, 11), inertia, 'o')\n",
    "    plt.plot(np.arange(1, 11), inertia, '-', alpha=0.5)\n",
    "    plt.xlabel('Number of Clusters'), plt.ylabel('Inertia')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-3-e1629ddd567c>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-3-e1629ddd567c>\"\u001B[1;36m, line \u001B[1;32m23\u001B[0m\n\u001B[1;33m    if i in range(0, 33):\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    algorithm_final = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, tol=0.0001, random_state=111,\n",
    "                             algorithm='elkan')\n",
    "\n",
    "    X3 = X[['Age', 'RNA EOT', 'RNA EF']].iloc[:, :].values\n",
    "    fig = plt.figure()\n",
    "    algorithm_final.fit(X3)\n",
    "    labels4 = algorithm_final.labels_\n",
    "    # print(labels3)\n",
    "    X['label4'] = labels4\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(xs=X['Age'], ys=X['RNA EOT'], zs=X['RNA EF'], marker='o', s=300,\n",
    "               c=X['label4'])\n",
    "    ax.set_xlabel('Age')\n",
    "    ax.set_ylabel('RNA EOT')\n",
    "    ax.set_zlabel('RNA EF')\n",
    "    plt.title('Clusters '+title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def discretization_Age(i):\n",
    "    if i in range(0, 33):\n",
    "        i = 0\n",
    "    elif i in range(33, 38):\n",
    "        i = 1\n",
    "    elif i in range(38, 43):\n",
    "        i = 2\n",
    "    elif i in range(43, 48):\n",
    "        i = 3\n",
    "    elif i in range(48, 53):\n",
    "        i = 4\n",
    "    elif i in range(53, 58):\n",
    "        i = 5\n",
    "    else:\n",
    "        i = 6\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def discretization_BMI(i):\n",
    "    if i in range(0, 18):\n",
    "        i = 0\n",
    "    elif i in range(18, 25):\n",
    "        i = 1\n",
    "    elif i in range(25, 30):\n",
    "        i = 2\n",
    "    elif i in range(30, 35):\n",
    "        i = 3\n",
    "    elif i in range(35, 40):\n",
    "        i = 4\n",
    "    elif i in range(53, 58):\n",
    "        i = 5\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def discretization_WBC(i):\n",
    "    if i in range(0, 4000):\n",
    "        i = 0\n",
    "    elif i in range(4000, 11000):\n",
    "        i = 1\n",
    "    elif i in range(11000, 12102):\n",
    "        i = 2\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def discretization_RBC(i):\n",
    "    if 0 <= i < 3000000:\n",
    "        i = 0\n",
    "    elif 3000000 <= i < 5000000:\n",
    "        i = 1\n",
    "    elif 5018452 > i >= 5000000:\n",
    "        i = 2\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def discretization_Plat(i):\n",
    "    if 93013 <= i < 100000:\n",
    "        i = 0\n",
    "    elif 100000 <= i < 225000:\n",
    "        i = 1\n",
    "    elif 225000 <= i < 226465:\n",
    "        i = 2\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def discretization_AST_ALT(i):\n",
    "    if 0 <= i < 20:\n",
    "        i = 0\n",
    "    elif 20 <= i <= 40:\n",
    "        i = 1\n",
    "    elif 40 < i <= 128:\n",
    "        i = 2\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def discretization_HGB(df):\n",
    "    print(df.loc[df.Gender == 1, 'HGB'])\n",
    "\n",
    "\n",
    "def discretization_RNA(i):\n",
    "    if 0 <= i <= 5:\n",
    "        i = 0\n",
    "    elif i > 5:\n",
    "        i = 1\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def discr_fun(X):\n",
    "    X['Age'] = X['Age'].apply(discretization_Age)\n",
    "    X['BMI'] = X['BMI'].apply(discretization_BMI)\n",
    "    X['WBC'] = X['WBC'].apply(discretization_WBC)\n",
    "    X['RBC'] = X['RBC'].apply(discretization_RBC)\n",
    "    X['Plat'] = X['Plat'].apply(discretization_Plat)\n",
    "    X['AST 1'] = X['AST 1'].apply(discretization_AST_ALT)\n",
    "    X['ALT 1'] = X['ALT 1'].apply(discretization_AST_ALT)\n",
    "    X['ALT 4'] = X['ALT 4'].apply(discretization_AST_ALT)\n",
    "    X['ALT 12'] = X['ALT 12'].apply(discretization_AST_ALT)\n",
    "    X['ALT 24'] = X['ALT 24'].apply(discretization_AST_ALT)\n",
    "    X['ALT 36'] = X['ALT 36'].apply(discretization_AST_ALT)\n",
    "    X['ALT 48'] = X['ALT 48'].apply(discretization_AST_ALT)\n",
    "    X['RNA Base'] = X['RNA Base'].apply(discretization_RNA)\n",
    "    X['RNA 4'] = X['RNA 4'].apply(discretization_RNA)\n",
    "    X['RNA 12'] = X['RNA 12'].apply(discretization_RNA)\n",
    "    X['RNA EOT'] = X['RNA EOT'].apply(discretization_RNA)\n",
    "    X['RNA EF'] = X[R'RNA EF'].apply(discretization_RNA)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def converting_to_0_and_1(X):\n",
    "    le = LabelEncoder()  # instanza che converte dal range [1,2,3,4] a [0,1,2,3]\n",
    "    # i valori variano e possono essere 1 o 2. Li converto in 0 e 1 per maggior praticità\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    # print(\"Gender array: \\n\"+str(X['Gender']))\n",
    "    X['Nausea or Vomiting'] = le.fit_transform(X['Nausea or Vomiting'])\n",
    "    X['Headache '] = le.fit_transform(X['Headache '])\n",
    "    X['Diarrhea '] = le.fit_transform(X['Diarrhea '])\n",
    "    X['Fatigue & generalized bone ache '] = le.fit_transform(X['Fatigue & generalized bone ache '])\n",
    "    X['Jaundice '] = le.fit_transform(X['Jaundice '])\n",
    "    X['Epigastric pain '] = le.fit_transform(X['Epigastric pain '])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def counting_features(Y):\n",
    "    if count_features:\n",
    "        # inizio conteggio per ogni classe, per vedere se è bilanciato\n",
    "        count_class_0 = 0\n",
    "        count_class_1 = 0\n",
    "        count_class_2 = 0\n",
    "        count_class_3 = 0\n",
    "\n",
    "        for i in Y:\n",
    "            # print(i)\n",
    "            if i == 0:\n",
    "                count_class_0 += 1\n",
    "            if i == 1:\n",
    "                count_class_1 += 1\n",
    "            if i == 2:\n",
    "                count_class_2 += 1\n",
    "            if i == 3:\n",
    "                count_class_3 += 1\n",
    "        print(\"samples of class 0: \" + str(count_class_0))\n",
    "        print(\"samples of class 1: \" + str(count_class_1))\n",
    "        print(\"samples of class 2: \" + str(count_class_2))\n",
    "        print(\"samples of class 3: \" + str(count_class_3))\n",
    "\n",
    "\n",
    "def normalization_and_standardization(X):\n",
    "    scaler = StandardScaler()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    names_cols = X.columns  # nomi delle colonne\n",
    "    X_std = pd.DataFrame(scaler.fit_transform(X[names_cols]), columns=names_cols)\n",
    "    X_scale = pd.DataFrame(min_max_scaler.fit_transform(X[names_cols]), columns=names_cols)\n",
    "    return X_std, X_scale\n",
    "\n",
    "\n",
    "def binarizing_problem(i):\n",
    "    if i == 1 or i == 2:\n",
    "        i = 0\n",
    "    if i == 3 or i == 4:\n",
    "        i = 1\n",
    "    return i\n",
    "\n",
    "\n",
    "def label_encoding(i_y):\n",
    "    i_y = i_y - 1\n",
    "    return i_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Dati di input\n",
    "    input_file = \"./HCV-Egy-Data/HCV-Egy-Data.csv\"\n",
    "    df = pd.read_csv(input_file, header=0)\n",
    "    print(\"Starting EDA...\")\n",
    "    EDA.analysis_dataset(df)\n",
    "    print(\"EDA finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    # df describe, descrive il dataset, inizio EDA\n",
    "\n",
    "    X = df.drop(columns='Baselinehistological staging')\n",
    "    X_not_discret = X.copy()\n",
    "    if discretization_bool:\n",
    "        X = discr_fun(X)\n",
    "\n",
    "    Y = df['Baselinehistological staging']\n",
    "    Y = Y.astype(int)  # converto in type int\n",
    "    X = converting_to_0_and_1(X)\n",
    "\n",
    "    X = X.drop(columns='HGB')\n",
    "    name_columns = X.columns\n",
    "    print(\"X:\\n\" + str(X))\n",
    "    # discretization_HGB(X) # TODO da rivedere\n",
    "\n",
    "    # stesso preprocessing per l'array di output\n",
    "    # Y = le.fit_transform(Y)\n",
    "\n",
    "    df = pd.concat([X, Y], axis=1)\n",
    "    print(\"DF after preprocessing: \\n\" + str(df))\n",
    "    counting_features(Y)  # conto le features\n",
    "    corr_df = df.corr()\n",
    "    print(\"The correlation DataFrame is:\")\n",
    "    print(corr_df, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    # list_corr=corr_df.abs().nlargest(28, Y)['Baselinehistological staging'].index\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    matrix = np.triu(corr_df, k=1)\n",
    "    sns.heatmap(corr_df, annot=True, cmap='coolwarm', square=True, linewidth=0.1, mask=matrix)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    X_std, X_min_max = normalization_and_standardization(X)\n",
    "\n",
    "    # TODO countplot, displot, pieplot, barplot, violin plot, pairplot\n",
    "    # countplot ==> mette a confronto della classe target, feature più rilevante\n",
    "\n",
    "    # print(names_cols + \"\\n\" + str(len(names_cols)))\n",
    "    # plot and save images, not preprocessing\n",
    "    # plot_metrics_for_each_features(names_cols, X, \"_not_preprocessing\")\n",
    "\n",
    "    # plot and save images, with standardizationount_class_0 = 0\n",
    "\n",
    "    # plot_metrics_for_each_features(names_cols, X_std, \"_standardized\")\n",
    "\n",
    "    # plot and save images, with min max scaler\n",
    "    # plot_metrics_for_each_features(names_cols, X_scale, \"min_max_scaler\")\n",
    "\n",
    "    if problem_is_binarized:\n",
    "        Y = Y.apply(binarizing_problem)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, random_state=0, train_size=0.70\n",
    "    )\n",
    "\n",
    "    # -----------------------Feature Selection---------------------------------\n",
    "    if problem_is_binarized:\n",
    "        clf_DT = Pipeline(\n",
    "            [('feature_selection', SelectFromModel(DecisionTreeClassifier())),\n",
    "             ('classification', DecisionTreeClassifier(random_state=0))])\n",
    "        clf_KNN = KNeighborsClassifier()\n",
    "    else:\n",
    "\n",
    "        clf_DT = Pipeline(\n",
    "            [('feature_selection', SelectFromModel(DecisionTreeClassifier())),\n",
    "             ('classification', DecisionTreeClassifier(random_state=0))])\n",
    "        clf_KNN = KNeighborsClassifier()\n",
    "\n",
    "    # --------------------------------END FEATURES SELCTION--------------------------------\n",
    "\n",
    "    if standardization:\n",
    "        normalization = False\n",
    "        X_train_std, X_test_std, Y_train, Y_test = train_test_split(\n",
    "            X_std, Y, random_state=0, train_size=0.70\n",
    "        )\n",
    "\n",
    "        clf_DT.fit(X_train_std, Y_train)\n",
    "        clf_KNN.fit(X_train_std, Y_train)\n",
    "        y_pred_1 = clf_DT.predict(X_test_std)\n",
    "        y_pred_2 = clf_KNN.predict(X_test_std)\n",
    "\n",
    "    elif normalization:\n",
    "        standardization = False\n",
    "\n",
    "        X_train_minmax, X_test_minmax, Y_train, Y_test = train_test_split(\n",
    "            X_min_max, Y, random_state=0, train_size=0.70\n",
    "        )\n",
    "\n",
    "        clf_DT.fit(X_train_minmax, Y_train)\n",
    "        clf_KNN.fit(X_train_minmax, Y_train)\n",
    "        y_pred_1 = clf_DT.predict(X_test_minmax)\n",
    "        y_pred_2 = clf_KNN.predict(X_test_minmax)\n",
    "    else:\n",
    "        # analisi bontà del clustering\n",
    "        clf_DT.fit(X_train, Y_train)\n",
    "        clf_KNN.fit(X_train, Y_train)\n",
    "        y_pred_1 = clf_DT.predict(X_test)\n",
    "        y_pred_2 = clf_KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    if discretization_bool:\n",
    "        clustering(X, \"with data not continues\")\n",
    "    clustering(X_not_discret, \"with data continues\")\n",
    "    # clustering(X_train)\n",
    "    # clustering(X_test)\n",
    "    # sns.displot(data=Y_test, x=Y_test.classes_)\n",
    "    # plt.show()\n",
    "    print(\"Preprocessing applied? \" + str(preproc))\n",
    "    print(\"Analysis with Discretization: \" + str(discretization_bool))\n",
    "    print(\"Problem is Binarized ?: \" + str(problem_is_binarized))\n",
    "    print(\"Standardization applied ? : \" + str(standardization))\n",
    "    print(\"Normalization applied? : \" + str(normalization))\n",
    "\n",
    "    print(\"accuracy score for KNeighbors: \" + str(accuracy_score(Y_test, y_pred_2)))\n",
    "    print(\"accuracy score for Decision tree: \" + str(accuracy_score(Y_test, y_pred_1)))\n",
    "    print(\"classification report for KNeighbors: \\n\" + str(classification_report(Y_test, y_pred_2)))\n",
    "    print(\"classification report for Decision tree:\\n \" + str(classification_report(Y_test, y_pred_1)))\n",
    "\n",
    "    # select_from_model(X, Y, clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}